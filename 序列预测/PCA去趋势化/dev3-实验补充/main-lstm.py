import numpy as np
import pickle

import tensorflow as tf
from tensorflow.contrib import rnn
import matplotlib.pyplot as plt


def lstm(layer_num, hidden_size, batch_size, output_size, lstm_x, keep_prob):
    def multi_cells(cell_num):
        # 多cell的lstm必须多次建立cell保存在一个list当中
        multi_cell = []
        for _ in range(cell_num):
            # **步骤2：定义LSTM_cell，只需要说明 hidden_size, 它会自动匹配输入的 X 的维度
            lstm_cell = rnn.BasicLSTMCell(num_units=hidden_size, forget_bias=1.0, state_is_tuple=True)

            # **步骤3：添加 dropout layer, 一般只设置 output_keep_prob
            lstm_cell = rnn.DropoutWrapper(cell=lstm_cell, input_keep_prob=1.0, output_keep_prob=keep_prob)
            multi_cell.append(lstm_cell)
        return multi_cell

    # **步骤4：调用 MultiRNNCell 来实现多层 LSTM
    mlstm_cell = rnn.MultiRNNCell(multi_cells(layer_num), state_is_tuple=True)

    # **步骤5：用全零来初始化state
    init_state = mlstm_cell.zero_state(batch_size, dtype=tf.float32)

    # **步骤6：调用 dynamic_rnn() 来让我们构建好的网络运行起来
    # ** 当 time_major==False 时， outputs.shape = [batch_size, time_step_size, hidden_size]
    # ** 所以，可以取 h_state = outputs[:, -1, :] 作为最后输出
    # ** state.shape = [layer_num, 2, batch_size, hidden_size]（中间的‘2’指的是每个cell中有两层分别是c和h）,
    # ** 或者，可以取 h_state = state[-1][1] 作为最后输出
    # ** 最后输出维度是 [batch_size, hidden_size]
    outputs, state = tf.nn.dynamic_rnn(mlstm_cell, inputs=lstm_x, initial_state=init_state, time_major=False)
    h_state = outputs[:, -1, :]  # 或者 h_state = state[-1][1]

    # 输出层
    # W_o = tf.Variable(tf.truncated_normal([hidden_size, output_size], stddev=0.1), dtype=tf.float32)
    # b_o = tf.Variable(tf.constant(0.1, shape=[output_size]), dtype=tf.float32)
    # y_pre = tf.add(tf.matmul(h_state, W_o), b_o)
    # tf.layers.dense是全连接层，不给激活函数，默认是linear function
    lstm_y_pres = tf.layers.dense(h_state, output_size)
    return lstm_y_pres


# 原始的data里面的数据格式是dataframe，arr改成了里面也是list
def transfer(data):
    vol_col_index = 1  # 找到流量对应的列
    height = len(data)
    width = data[0].shape[0]
    arr = np.zeros((height, width))
    for i in range(height):
        for j in range(width):
            arr[i, j] = data[i].iloc[j, vol_col_index]
    return arr


def createdataset(data):
    dataset = []
    for road in data:  # 对于某一条路的数据
        dataset.append(transfer(road))
    return dataset


def myload():
    filename = 'dump2.txt'
    #f = open('./data/' + filename, 'rb')
    abspath='C:/Users/wwwa8/Documents/GitHub/Machine-Learning/序列预测/PCA去趋势化/dev2/data/'
    f = open(abspath + filename, 'rb')
    data = pickle.load(f)
    f.close()
    # print (data)   # 路段数 * 每个路段的信息（df的数据结构）
    return data


def split_dataset(dataset):
    trainX = []
    trainY = []
    trainX_len = 2  # 使用3天预测一天
    trainY_len = 1  # 使用3天预测一天
    days = dataset[0].shape[0]
    for road in dataset:
        trainX_per_road = []
        trainY_pre_road = []
        for i in range(0, days - (trainX_len + trainY_len - 1)):
            trainX_per_road.append(road[i:i + trainX_len])
            trainY_pre_road.append(road[i + trainX_len:i + trainX_len + trainY_len])
        trainX.append(trainX_per_road)
        trainY.append(trainY_pre_road)
    return np.array(trainX), np.array(trainY)



name_count=0
def show1_ticks(arr_true,arr_prediction):
    #改变x轴的刻度
    x_kedu=[0,4,8,12,16,20,24]
    orig_ticks = [i*20 for i in x_kedu]
    new_ticks = x_kedu
    plt.xticks(orig_ticks,new_ticks)
    #改变y轴的刻度
    y_kedu=[0,5,10,15,20,25]
    y_orig_ticks = y_kedu
    y_new_ticks =y_kedu
    plt.yticks(y_orig_ticks,y_new_ticks)
    
    plt.xlabel("Time (hour)")
    plt.ylabel("Traffic volume")
    plt.plot(arr_true,'b-',label='true')
    plt.plot(arr_prediction,'r-',label='prediction')
    plt.legend(loc='upper right')
    plt.grid()
    #plt.show()
    global name_count
    plt.savefig(str(name_count)+".png")
    plt.close()
    name_count+=1


if __name__ == "__main__":
    time_step = 2
    data = myload()  # data是一个list，里面是df格式的数据
    # transfer
    dataset = createdataset(data)                  # dataset 的格式是 （路段 * 每一天 * 一天内的数据）20 * 14 * 480

    #dataset_main, dataset_rest = use_pca(dataset)  # dataset_main = 路段 * 每一个路段里面的主成分 ; dataset_rest = 路段 * 每一个路段里面的偏差
    road_num = len(dataset)    
    days, dnum =dataset[0].shape #days =14,dnum=480

    #y_main = dataset_main[0][time_step:days, :]    # y_main的主成分[2~14] shape 12 * 2 * 480
    #将偏差数据 分割 成输入和输出
    trainX, trainY = split_dataset(dataset)   #trainX 的shape 20 * 12 * 2 *480 ； trainY 的shape 20 * 12 * 1 *480

    #train_x_raw = trainX[0]                                       # 取了第一条路段来进行预测 train_x_raw的sahpe ：12 * 2 *480
    #train_y_raw = np.reshape(trainY[0], (days - time_step, 480))  # train_y_raw的shape 12 * 480
    train_x_raw = np.reshape(trainX, (road_num * (days - time_step),time_step,dnum))   #train_x_raw (20 * 12,2,480)
    train_y_raw = np.reshape(trainY, (road_num * (days - time_step),dnum))             #train_y_raw (20 * 12,1,480)

    # 归一化
    x_max = train_x_raw.max()
    x_min = train_x_raw.min()
    y_max = train_y_raw.max()
    y_min = train_y_raw.min()

    train_x = (train_x_raw - x_min) / (x_max - x_min)
    train_y = (train_y_raw - y_min) / (y_max - y_min)

    # 构造训练和测试集
    test_split_rate=0.25
    total_len = train_x.shape[0]
    train_len = int(total_len * (1-test_split_rate))
    test_len = int(total_len * test_split_rate)

    test_x = train_x[train_len:]
    test_y = train_y[train_len:]
    #train_x =train_x[0:train_len]
    #train_y =train_y[0:train_len]
    train_x =train_x
    train_y =train_y

    # 放入lstm训练
    # lstm的hyper-parameter
    hidden_size = 200
    layer_num = 1
    max_epoch = int(2000 * 6)  #6
    dropout_keep_rate = 0.9    #0.9 

    # 根据输入数据来决定，train_num训练集大小,input_size输入维度
    train_num, time_step_size, input_size = train_x.shape     # sahpe ：12 * 2 *480
    # output_size输出的结点个数
    _, output_size = train_y.shape

    # **步骤1：LSTM 的输入shape = (batch_size, time_step_size, input_size)，输出shape=(batch_size, output_size)
    x_input = tf.placeholder(tf.float32, [None, time_step_size, input_size])
    y_real = tf.placeholder(tf.float32, [None, output_size])

    # dropout的留下的神经元的比例
    keep_prob = tf.placeholder(tf.float32, [])

    # 在训练和测试的时候，我们想用不同的 batch_size.所以采用占位符的方式
    batch_size = tf.placeholder(tf.int32, [])  # 注意类型必须为 tf.int32

    pre_layer_hidden_num = 0
    pre_layer_hidden_size = 0
    hide_output = x_input

    y_pred = lstm(layer_num, hidden_size, batch_size, output_size, hide_output, keep_prob)
    # 损失和评估函数
    mse = tf.losses.mean_squared_error(y_real, y_pred)
    train_op = tf.train.AdamOptimizer(1e-3).minimize(mse)

    # 设置 GPU 按需增长
    #config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)
    # config.gpu_options.allow_growth = True
    #sess = tf.Session(config)
    # cpu 
    sess = tf.Session()

    # 初始化变量
    sess.run(tf.global_variables_initializer())

    mre_result = []
    mae_result = []
    rmse_result = []

    # 获得训练的指标
    def get_metrics(y, pred_y):
        #替换为0的元素
        y_mean=np.mean(y)
        y[y==0.00] =y_mean
        mre = np.mean(np.abs(y - pred_y) / y)
        mae = np.mean(np.abs(y - pred_y))
        rmse = np.sqrt(np.mean(np.square(y - pred_y)))
        return mre, mae, rmse

    def cal_mre(y,y_pre):
        y[y < 0.1] = 0.1
        diff = np.abs(y-y_pre)
        mre = np.mean(diff/y)
        print("cal mre is : ", mre)

    def print_to_console(i, train_y ,train_y_pred,flag_istrain):
        train_y_pred_real = train_y_pred * (y_max - y_min) + y_min  # 反归一化
        train_y_real = train_y * (y_max - y_min) + y_min            # train_y 是真实的y值，堆train_y 进行反归一化

        #plt.plot(range(dnum), train_y_real[0], 'b-',label='true')        # 实际用蓝色
        #plt.plot(range(dnum), train_y_pred_real[0], 'r-',label='prediction')   # 预测用红色
        #plt.show()
        """
        if (flag_istrain==1):
            plt.savefig("train"+str(i)+".png")
        else:
            plt.savefig("test" +str(i)+".png")
        """
        train_mre, train_mae, train_rmse = get_metrics(train_y_real, train_y_pred_real)
        if(flag_istrain==1):
            print("epoch {} train : {} {} {} ".format(i, train_mre, train_mae, train_rmse))
        else:
            print("epoch {} test : {} {} {} ".format(i, train_mre, train_mae, train_rmse))

    def cal_total_inlstm(y_raw_test,y_pre_test_real):  #true ,pred
        test_len = y_raw_test.shape[0]
        test_mre=0.0
        test_mae=0.0
        test_rmse=0.0
        for i in range(0,test_len):
            res = get_metrics(y_raw_test[i],y_pre_test_real[i])
            test_mre+=res[0]
            test_mae+=res[1]
            test_rmse+=res[2]
        return test_mre/test_len,test_mae/(test_len+1),test_rmse/test_len

    for i in range(1, max_epoch + 1):
        feed_dict = {x_input: train_x, y_real: train_y, keep_prob: dropout_keep_rate, batch_size: train_num}
        sess.run(train_op, feed_dict=feed_dict)
        show_iter=200
        if i % show_iter == 0:
            feed_dict = {x_input: train_x, y_real: train_y, keep_prob: 1.0, batch_size: train_num}
            train_y_pred = sess.run(y_pred, feed_dict=feed_dict)
            #print ("train_y_pred : ",train_y_pred.shape)    #(9,480)
            print_to_console(i,train_y, train_y_pred,1)
        if i % show_iter ==0:
            feed_dict = {x_input: test_x, y_real: test_y, keep_prob: 1.0, batch_size: test_len}
            test_y_pred = sess.run(y_pred, feed_dict=feed_dict)
            #print ("test_y_pred : ",test_y_pred.shape)      #(3,480)
            print_to_console(i, test_y,test_y_pred,0)
    
    #train
    feed_dict = {x_input: train_x, y_real: train_y, keep_prob: 1.0, batch_size: train_num}    
    train_y_pred = sess.run(y_pred, feed_dict=feed_dict)
    train_y_pred = np.reshape(train_y_pred,(road_num,int(days-time_step),dnum))
    train_y      = np.reshape(train_y,     (road_num,int(days-time_step),dnum))

    #test
    feed_dict = {x_input: test_x, y_real: test_y, keep_prob: 1.0, batch_size: test_len}
    test_y_pred = sess.run(y_pred, feed_dict=feed_dict)
    test_y_pred = np.reshape(test_y_pred,  (road_num,int(days*test_split_rate),dnum))  #(20,3,480)
    test_y = np.reshape(test_y,(road_num,int(days*test_split_rate),dnum))            #(20,3,480)
    
    # 反归一化
    test_y_pred = test_y_pred* (y_max - y_min) + y_min
    test_y = test_y * (y_max - y_min) + y_min
    train_y_pred = train_y_pred* (y_max - y_min) + y_min
    train_y = train_y * (y_max - y_min) + y_min
    
    #plt.plot(range(dnum), test_y[0][0], 'b-',label='true')              # 实际用蓝色
    #plt.plot(range(dnum), test_y_pred[0][0], 'r-',label='prediction')   # 预测用红色
    #plt.legend(loc='upper right')
    #plt.show()
    show1_ticks(train_y[0][0],train_y_pred[0][0])
    show1_ticks(train_y[0][1],train_y_pred[0][1])
    show1_ticks(train_y[0][2],train_y_pred[0][2])

    show1_ticks(test_y[0][0],test_y_pred[0][0])
    show1_ticks(test_y[0][1],test_y_pred[0][1])
    show1_ticks(test_y[0][2],test_y_pred[0][2])
    print("mre, mae, rmse : ",cal_total_inlstm(test_y,test_y_pred))



"""
hidden_size = 200
layer_num = 1
max_epoch = int(2000 * 10)
dropout_keep_rate = 0.9   #dropout_keep_rate 降低，效果就差一点

epoch 200 train : 0.39798301925076296 1.8144739025190875 2.737262137635115
epoch 200 test : 0.3925649338403189 1.980381288756773 3.0827433023933053
epoch 400 train : 0.38364784932343515 1.713024023108663 2.5907330325078255
epoch 400 test : 0.3779529825093189 1.8418526435114735 2.889717685749368
epoch 600 train : 0.3532076163862305 1.6121853083320166 2.479852210189239
epoch 600 test : 0.34745315007446903 1.7109425060322418 2.738597342954611
epoch 800 train : 0.33598718380845544 1.5575703670207024 2.4196838316535296
epoch 800 test : 0.33102298962297044 1.6466822826045353 2.666239228827481
epoch 1000 train : 0.33548292503146226 1.5038508675445128 2.3617315278879207
epoch 1000 test : 0.32878442092014065 1.5831865037326451 2.6001746506452847
epoch 1200 train : 0.3197845503026627 1.4481111274776162 2.3076822852051664
epoch 1200 test : 0.3119409785020127 1.5216803610568252 2.539814680739114
epoch 1400 train : 0.2958856615371342 1.3888374488116781 2.252741760466906
epoch 1400 test : 0.28612602482673083 1.443550038646813 2.4660738614292703
epoch 1600 train : 0.28123831524120924 1.3493299787724873 2.225546036619812
epoch 1600 test : 0.2705506400566686 1.4111015794212234 2.4459354258846155
epoch 1800 train : 0.2733711417093635 1.28104743362514 2.1607185332441476
epoch 1800 test : 0.26116030719880945 1.31714665827184 2.361571473632453
epoch 2000 train : 0.27098812952719253 1.2619070001491104 2.1407361904259616
epoch 2000 test : 0.2561627049215643 1.3037181528811572 2.3444005360265034
epoch 2200 train : 0.24903544775754777 1.2191571342461645 2.1127954581206336
epoch 2200 test : 0.23342284257513402 1.2520027439579313 2.311366552696613
epoch 2400 train : 0.24023745339917918 1.1589938681748764 2.0663817841522185
epoch 2400 test : 0.2235595956021224 1.174439267752846 2.254397954716117
epoch 2600 train : 0.23119122069815334 1.1762857839355607 2.0888257066261
epoch 2600 test : 0.21469384540978076 1.2224508455077507 2.300272205167223
epoch 2800 train : 0.2311158437081879 1.1040668450844082 2.0289444919512163
epoch 2800 test : 0.21350657364425954 1.1191165084525345 2.2204119074861546
epoch 3000 train : 0.22385987688154688 1.068310435063662 2.0031827192921408
epoch 3000 test : 0.2071327227187568 1.0811070802143705 2.1939652621470036
epoch 3200 train : 0.2110980636925057 1.0427808620561685 1.9911473762302698
epoch 3200 test : 0.19482943330557198 1.065329670208399 2.1896090581959133
epoch 3400 train : 0.20439564585277853 1.038942146384513 1.9912925837231779
epoch 3400 test : 0.18802117227652743 1.058181829051879 2.184415146140406
epoch 3600 train : 0.20285657116697453 0.9798648368757243 1.9512235233797413
epoch 3600 test : 0.18688127367130714 0.9954308131395616 2.148467215686238
epoch 3800 train : 0.19887497799208642 0.9674991813295094 1.9416096441038566
epoch 3800 test : 0.1825676912033511 0.9800540890102313 2.134070575949774
epoch 4000 train : 0.1982210250905738 0.9683468737230471 1.9455906022514853
epoch 4000 test : 0.1828738817935321 0.9885314443248259 2.1447443078685544
epoch 4200 train : 0.1921208046621289 0.9341652246980968 1.9291473801063859
epoch 4200 test : 0.17669179016606212 0.9455807906313863 2.1224052564352514
epoch 4400 train : 0.1810951172180863 0.924585943136401 1.9287143789112366
epoch 4400 test : 0.16579457164733696 0.9454205656203514 2.13009885365506
epoch 4600 train : 0.17759338786444748 0.8827055864302596 1.9021993096408232
epoch 4600 test : 0.16373452500198957 0.8982013053902796 2.101415093568664
epoch 4800 train : 0.17474551911422834 0.8697135807716512 1.8978746199822363
epoch 4800 test : 0.16152631131838183 0.8823396757884463 2.094198674119614
epoch 5000 train : 0.1706000619362463 0.8642919580089441 1.8970898646330887
epoch 5000 test : 0.15753538088716929 0.8786548125138832 2.092761848519943
epoch 5200 train : 0.1712119658802744 0.8545354697382177 1.894695281478917
epoch 5200 test : 0.15789884431190288 0.873057376902633 2.0952236390981005
epoch 5400 train : 0.16794404749906727 0.8407832347176005 1.8871857378704915
epoch 5400 test : 0.15412234971272054 0.8562775972533793 2.0860448478475107
epoch 5600 train : 0.1625233176193142 0.8224165428483207 1.8793117203659184
epoch 5600 test : 0.14865945666323105 0.8345462562984105 2.073959685667136
epoch 5800 train : 0.16230621217670985 0.8245971494762498 1.879954744163689
epoch 5800 test : 0.14993089859749645 0.8388640685387602 2.0772628572617866
epoch 6000 train : 0.15897240629288922 0.8383588881897936 1.889432463894452
epoch 6000 test : 0.1467441564176491 0.8646343185080001 2.0948054134231477
epoch 6200 train : 0.1581189507576361 0.8182657654587873 1.8820742696646902
epoch 6200 test : 0.14567450272071034 0.8400026067311773 2.0832587835776777
epoch 6400 train : 0.1544077154110386 0.7787399903865089 1.861458469359221
epoch 6400 test : 0.14090059562315269 0.7886563000964558 2.059718788504594
epoch 6600 train : 0.1533840780129031 0.7829848482604879 1.8627651269258574
epoch 6600 test : 0.14063096478150422 0.794075459771741 2.061049866699811
epoch 6800 train : 0.1567740610661872 0.7930309190105517 1.867620517491318
epoch 6800 test : 0.14264480392179557 0.8179693543548071 2.0722765010949367
epoch 7000 train : 0.14849810366111807 0.7886103113827619 1.8656711875441052
epoch 7000 test : 0.1343756869712028 0.7918382782960031 2.060117638493019
epoch 7200 train : 0.15128212134024388 0.7594876317593521 1.8511747685612228
epoch 7200 test : 0.13964459776807323 0.7723107346313146 2.0500312861703387
epoch 7400 train : 0.15204138173895076 0.7589987794168388 1.8493382753371599
epoch 7400 test : 0.1382322848752084 0.774642198037141 2.0507745692923067
epoch 7600 train : 0.1491383219201202 0.7559753743889462 1.856018202277452
epoch 7600 test : 0.13688572619773665 0.7752753367799331 2.0577017102998036
epoch 7800 train : 0.14835442794378487 0.7669258733516788 1.8577017980447479
epoch 7800 test : 0.13586704325873666 0.7835881710417968 2.055808521692036
epoch 8000 train : 0.1471966376976472 0.7409233879535311 1.8458272971127452
epoch 8000 test : 0.13449769679840867 0.7525800719979552 2.043415238610942
epoch 8200 train : 0.14459028890088524 0.7499995992355446 1.8523520845977686
epoch 8200 test : 0.13161812679962173 0.755677502653185 2.046882684003515
epoch 8400 train : 0.15009011735831143 0.7505946662155515 1.8515671345249902
epoch 8400 test : 0.13740870507703748 0.7685612490412839 2.051356838329426
epoch 8600 train : 0.14369902641149243 0.7297559589431216 1.8434436621065258
epoch 8600 test : 0.1319661647453438 0.7431313177374895 2.0416349667361646
epoch 8800 train : 0.14678764034462916 0.7384667528121415 1.8410309420405433
epoch 8800 test : 0.13382210227803049 0.7490917063252226 2.039641500208142
epoch 9000 train : 0.14236800880211437 0.7516250260911383 1.8545423812404853
epoch 9000 test : 0.1303910854727086 0.7728921072229181 2.056862089742984
epoch 9200 train : 0.13904054915901043 0.7447350806833046 1.8496708829831856
epoch 9200 test : 0.12598588153436932 0.758064711826485 2.0506359036478155
epoch 9400 train : 0.13875567614091017 0.7396487447655925 1.8480551895753705
epoch 9400 test : 0.12588378134185862 0.7506177062284036 2.045940280202408
epoch 9600 train : 0.1473937353698146 0.7437100834184805 1.8493239551245069
epoch 9600 test : 0.1369517420737929 0.7620283780817321 2.0474300686050726
epoch 9800 train : 0.14153307665750042 0.7247919715033818 1.8379099084869008
epoch 9800 test : 0.12939871198038208 0.7418672667196612 2.0399849253101316
epoch 10000 train : 0.13884945365408322 0.7193956960836121 1.845810516419187
epoch 10000 test : 0.12565150555830523 0.7380639941237535 2.046838263856642
epoch 10200 train : 0.14239693919750782 0.7294629694897239 1.8403121381151601
epoch 10200 test : 0.13109301947946436 0.7426301464267456 2.039195900411708
epoch 10400 train : 0.13869843334580723 0.7266536538527374 1.8450330574466924
epoch 10400 test : 0.12647746967384987 0.7438455117733136 2.0459213113445807
epoch 10600 train : 0.14712532764321626 0.7609381832074501 1.8549196414228144
epoch 10600 test : 0.13497083945361993 0.7841189429072518 2.05877055217485
epoch 10800 train : 0.13862282963615263 0.7429317257731302 1.855756519635022
epoch 10800 test : 0.1257999540134593 0.7655413143416336 2.0562736960529056
epoch 11000 train : 0.1393732302264948 0.7115155210818709 1.834197597980124
epoch 11000 test : 0.12734528678050627 0.7223668896000919 2.0316580648054416
epoch 11200 train : 0.13762390309256425 0.7179452411964837 1.8431638206837442
epoch 11200 test : 0.12469606253874287 0.7322027358109912 2.0418691553322263
epoch 11400 train : 0.1445779185467089 0.7216314796343505 1.834564104176666
epoch 11400 test : 0.12995479763783915 0.7346737566736771 2.0358567010176847
epoch 11600 train : 0.13582951651498637 0.7056575557423408 1.8364864712504143
epoch 11600 test : 0.12462318768935407 0.719256021738195 2.0368430858757365
epoch 11800 train : 0.13738949695730174 0.7077699678373156 1.8362228931963538
epoch 11800 test : 0.12533307466763702 0.7262039353892006 2.0374738643257855
epoch 12000 train : 0.1333790702137449 0.7098480455630951 1.8411247644709174
epoch 12000 test : 0.12078554779789155 0.7212781734143967 2.0412922906903694
epoch 12200 train : 0.13853186003910561 0.7197168308335282 1.8457087483441152
epoch 12200 test : 0.12647152807254752 0.7309489469060806 2.0421192822761025
epoch 12400 train : 0.13751165796514173 0.6985342425317409 1.8306473589884507
epoch 12400 test : 0.125400855868693 0.7195852549494989 2.032600043117352
epoch 12600 train : 0.1350083197653536 0.6972828890660253 1.8309562369835999
epoch 12600 test : 0.12211072611388009 0.7045329575128313 2.0313498036473976
epoch 12800 train : 0.13655631494816858 0.7099940525890702 1.8363123625005267
epoch 12800 test : 0.12400270258537595 0.7214149492229922 2.035692785842244
epoch 13000 train : 0.13461723751884114 0.7004277161463431 1.8357733156114846
epoch 13000 test : 0.12210131085535612 0.713574158061351 2.0355883114081244
epoch 13200 train : 0.1374320839451719 0.719937672522651 1.8440423363109841
epoch 13200 test : 0.12385084993867336 0.7327316612062634 2.0431662399947275
epoch 13400 train : 0.13430064310605686 0.6975166225115021 1.8325420520889468
epoch 13400 test : 0.12318712015622324 0.7145616545568252 2.033452214657543
epoch 13600 train : 0.13759506811129807 0.7078787347143859 1.8353733874760092
epoch 13600 test : 0.1247535118494531 0.7200538719173167 2.036951753585731
epoch 13800 train : 0.13319923629115682 0.6990024510014825 1.8352856485551083
epoch 13800 test : 0.12080922077252937 0.7080463688310357 2.0342993848437056
epoch 14000 train : 0.13274286643124536 0.6964224387022157 1.8343305339193934
epoch 14000 test : 0.12059313663059405 0.7042833548745799 2.0330132265839547
epoch 14200 train : 0.13355311897523364 0.6943370893245552 1.8312702876776727
epoch 14200 test : 0.12084422456288983 0.7040127773020322 2.0291462781906984
epoch 14400 train : 0.13325019229925367 0.7008866873819545 1.8364821287659925
epoch 14400 test : 0.12049825803987749 0.7081258580039124 2.034986471075454
epoch 14600 train : 0.13293880750865417 0.7151016909847225 1.8409185105681771
epoch 14600 test : 0.12044665963619379 0.7223381433208771 2.0364528042055516
epoch 14800 train : 0.13150628636561856 0.6957477869961848 1.8315337925179316
epoch 14800 test : 0.1192966759985954 0.7103190736600542 2.033084848927186
epoch 15000 train : 0.1353349254880954 0.7007814152566673 1.8346174898642653
epoch 15000 test : 0.12143764587356093 0.7035460678562349 2.0305776626389487
epoch 15200 train : 0.13573839223026382 0.6943390423332498 1.8325444066288736
epoch 15200 test : 0.1229185262715612 0.7113991177622799 2.0320977969454654
epoch 15400 train : 0.13283940854087803 0.6920179499139807 1.8322840828003644
epoch 15400 test : 0.1207826022222978 0.7040250189839452 2.0312106802830043
epoch 15600 train : 0.1328190363519031 0.7002733763386444 1.8375210207519428
epoch 15600 test : 0.12117451136517764 0.7191718311119042 2.038130986169676
epoch 15800 train : 0.13479617487476367 0.7038429782267367 1.8374658737320382
epoch 15800 test : 0.12453294681010044 0.7331247361886424 2.0433719772374928
epoch 16000 train : 0.13104490132083754 0.6930291744356082 1.8327801696559893
epoch 16000 test : 0.11809001701830457 0.6994197826016395 2.0298537763644506
epoch 16200 train : 0.1296765431560534 0.6806210284806433 1.8275674874485142
epoch 16200 test : 0.11757080396960536 0.6915729309595621 2.025842118289391
epoch 16400 train : 0.13252174183603338 0.6955365112660458 1.8337571347644492
epoch 16400 test : 0.12039112473795394 0.7058317822940169 2.03356875691494
epoch 16600 train : 0.14342826530707006 0.7238583402042793 1.8363909874417708
epoch 16600 test : 0.12964653945868043 0.7325264951089522 2.0358118197367903
epoch 16800 train : 0.13475143877662837 0.6916643833137951 1.829352551356629
epoch 16800 test : 0.12317427065062168 0.7059275039078832 2.029696358254857
epoch 17000 train : 0.13423393161965017 0.7165348189530311 1.8415623301043615
epoch 17000 test : 0.12225444478886589 0.7434961989528149 2.0468636373680162
epoch 17200 train : 0.1325204213598765 0.6900278676768705 1.8346928388582462
epoch 17200 test : 0.12110431252327061 0.7001124409136636 2.032837875310593
epoch 17400 train : 0.13416875673019618 0.7192220684577033 1.8427207106401273
epoch 17400 test : 0.12299519652125747 0.7280555053179618 2.0395697878903327
epoch 17600 train : 0.12796910939854794 0.6827345536584861 1.8316813693722576
epoch 17600 test : 0.11549439262440711 0.6912283805677267 2.0281697919166426
epoch 17800 train : 0.13082945332092838 0.6802354213047732 1.8271072272152995
epoch 17800 test : 0.11823966042021598 0.6905719756873199 2.0251651517368985
epoch 18000 train : 0.12968653382111808 0.6843956589784996 1.8297730179221348
epoch 18000 test : 0.1170316183857202 0.6971446975103325 2.0294857291884707
epoch 18200 train : 0.12882102287183497 0.6763871510662994 1.828502313223391
epoch 18200 test : 0.11676945964524618 0.687077906420504 2.0268740048136555
epoch 18400 train : 0.1297929573070731 0.6893666537458638 1.8325355981216163
epoch 18400 test : 0.11585052678659663 0.706502560803268 2.035673187019449
epoch 18600 train : 0.13529353492463653 0.6943667259632925 1.8300202220975084
epoch 18600 test : 0.12364649040363905 0.7135800670933581 2.033061973315084
epoch 18800 train : 0.12909116008486765 0.6956906335857804 1.8372756033654754
epoch 18800 test : 0.11631501605799213 0.7239107542366623 2.042593809681482
epoch 19000 train : 0.13059514114735793 0.6874503823559716 1.8280702714996857
epoch 19000 test : 0.11668231116064211 0.6981651208199927 2.0281105135148607
epoch 19200 train : 0.13399250294379972 0.7070163959248038 1.8387296380978486
epoch 19200 test : 0.12142030728510037 0.7175725598782242 2.0383276029479696
epoch 19400 train : 0.13063528562676735 0.7020036104016433 1.8400746483742145
epoch 19400 test : 0.11702145047501372 0.7199777599214239 2.04254781294608
epoch 19600 train : 0.12893719333239548 0.6835586781289593 1.8324471928137023
epoch 19600 test : 0.11593060337711657 0.6921355830905153 2.0297136807887424
epoch 19800 train : 0.12853487423020227 0.6897342423175535 1.8350324321914961
epoch 19800 test : 0.11612066544408584 0.7024936136629573 2.0345234142699073
epoch 20000 train : 0.13720405660952403 0.6988310584941588 1.8318585937119094
epoch 20000 test : 0.12257196057393169 0.7032812590759051 2.0289320126556754



hidden_size = 200
layer_num = 1
max_epoch = int(2000 * 10)
dropout_keep_rate = 0.9

epoch 200 train : 0.41507158288473 1.8929657225784178 2.855774406156602
epoch 200 test : 0.4132121619320544 2.105662503461924 3.2637666481203156
epoch 400 train : 0.3812449768063483 1.7897985683123785 2.7131077498320137
epoch 400 test : 0.37688294203765016 1.9543729914487644 3.054023880470206
epoch 600 train : 0.37386033447970546 1.750499758535603 2.661763398921021
epoch 600 test : 0.367690096927521 1.900080884641831 2.99236515419708
epoch 800 train : 0.3490387056736635 1.712243933591629 2.6162793143318366
epoch 800 test : 0.3426215488476694 1.8668775675464473 2.9405110991913297
epoch 1000 train : 0.3621901767147528 1.6760009439468193 2.5600573346489175
epoch 1000 test : 0.3532071944985285 1.795901368511444 2.8520407141904034
epoch 1200 train : 0.3479108933985724 1.6394255101037458 2.5233242259646254
epoch 1200 test : 0.33951243513943713 1.7480087045361492 2.8076713018120034
epoch 1400 train : 0.3210264806637669 1.6191275665042657 2.508781011510117
epoch 1400 test : 0.3155907032140085 1.7432103855202201 2.7969883025073505
epoch 1600 train : 0.3335916553593158 1.5789764295680013 2.4539550884623234
epoch 1600 test : 0.32650934170120266 1.672764819747778 2.7166023657564136
epoch 1800 train : 0.3423590995476494 1.5812938419580629 2.448956409758251
epoch 1800 test : 0.33664047909647843 1.6792461382805721 2.712073564289215
epoch 2000 train : 0.34385794199858 1.579048924734911 2.4449922892311746
epoch 2000 test : 0.3371480648310496 1.6701797135598653 2.696920283716657
epoch 2200 train : 0.3164352496783839 1.5342460967190603 2.408961666456828
epoch 2200 test : 0.3082297613971179 1.6090348745434098 2.653317276909937
epoch 2400 train : 0.31688618225650583 1.5150973322887136 2.3863915008710435
epoch 2400 test : 0.31096185528563586 1.6156030117833964 2.6484131091508534
epoch 2600 train : 0.3063741119423281 1.552947478808936 2.4344973120816915
epoch 2600 test : 0.29811351481923953 1.6746591195141132 2.722759566908907
epoch 2800 train : 0.31702290042797376 1.4949785655393006 2.360106811104135
epoch 2800 test : 0.3092066017947592 1.5859629626295482 2.610896806768169
epoch 3000 train : 0.3075125887333913 1.504411658467213 2.374478659794531
epoch 3000 test : 0.29945803294748014 1.640148939491208 2.664175561123412
epoch 3200 train : 0.312564524843688 1.4719143146640714 2.3362491891317183
epoch 3200 test : 0.3033883732484258 1.5425401901257283 2.572297751244083
epoch 3400 train : 0.31249972939077686 1.4805272090618022 2.3427761161703744
epoch 3400 test : 0.30321587603875766 1.560531929461264 2.58416306552745
epoch 3600 train : 0.31852997020424323 1.4727150071966926 2.331390812701384
epoch 3600 test : 0.30857940550980467 1.5465130128780402 2.5727408087473536
epoch 3800 train : 0.30762669676605453 1.4487695285665978 2.3123539603569108
epoch 3800 test : 0.29677215782781624 1.5082319147177723 2.541506328353748
epoch 4000 train : 0.32165939901384943 1.4850936924027534 2.341468344449412
epoch 4000 test : 0.31150180464940075 1.5781084217509478 2.5923180263196404
epoch 4200 train : 0.3071749304880257 1.4436362874439785 2.305675757628385
epoch 4200 test : 0.29650515226652374 1.5191567168487892 2.54283699245043
epoch 4400 train : 0.29907692013834075 1.439125384211688 2.302571133596366
epoch 4400 test : 0.2884701909022899 1.5061351786427744 2.5318112985148047
epoch 4600 train : 0.3076223794095154 1.4409912334162758 2.300086179848135
epoch 4600 test : 0.29759958292611993 1.5141293160703262 2.535874994399687
epoch 4800 train : 0.30488693505068354 1.4396328849159445 2.2977790025007274
epoch 4800 test : 0.29195656582037127 1.4935177465080658 2.5166541231308486
epoch 5000 train : 0.31016223176236346 1.428156269338023 2.2866140190196993
epoch 5000 test : 0.2980142251816142 1.4886063447291809 2.5095852559803693
epoch 5200 train : 0.294764089981929 1.4750964464277223 2.3363823297866335
epoch 5200 test : 0.28316612599905167 1.5352650134121466 2.558652925734948
epoch 5400 train : 0.3096093958751796 1.446733056478171 2.306698002108836
epoch 5400 test : 0.29918204943140525 1.5430146781036633 2.5709417705401294
epoch 5600 train : 0.29305588063230253 1.4122783235220921 2.272262927143337
epoch 5600 test : 0.28255871435583085 1.4794590630255855 2.502185415843553
epoch 5800 train : 0.296498893577883 1.4197065226022831 2.2781570598326972
epoch 5800 test : 0.28608057018042715 1.497377227510609 2.517199437919154
epoch 6000 train : 0.29961221786147363 1.402540837867004 2.259849243247592
epoch 6000 test : 0.28861668388891387 1.4700033243544277 2.4900914760567785
epoch 6200 train : 0.30445869647261753 1.3992248728118615 2.2581729969055453
epoch 6200 test : 0.29462302337329105 1.449124249643242 2.474520549155332
epoch 6400 train : 0.30544784411283865 1.398668317495228 2.2508413105307383
epoch 6400 test : 0.2938042253909372 1.4432834733817808 2.459710642585822
epoch 6600 train : 0.3022834198133292 1.4132871138193537 2.265889467699023
epoch 6600 test : 0.2917986614034527 1.5046333542720716 2.514024548651664
epoch 6800 train : 0.30545163648496587 1.4050390855768353 2.2571466147748414
epoch 6800 test : 0.2983783230198873 1.48901992282487 2.494728822216349
epoch 7000 train : 0.2960397112425154 1.397518643379558 2.2541723413517096
epoch 7000 test : 0.2874428604225551 1.475989291543902 2.4852872258150227
epoch 7200 train : 0.3059646938135823 1.394349455103369 2.24749866028457
epoch 7200 test : 0.2927939476364329 1.446992280150686 2.4689153617505357
epoch 7400 train : 0.296393057964237 1.3982033239103286 2.24886410768856
epoch 7400 test : 0.28394995475528956 1.4531876386910585 2.4573735473410694
epoch 7600 train : 0.2955362506735442 1.379581050392784 2.2374995639306436
epoch 7600 test : 0.28518450448330784 1.439967350154631 2.4569891645595576
epoch 7800 train : 0.29641621228862197 1.3784369140370356 2.2331771243890706
epoch 7800 test : 0.2845113351969688 1.4266387983692344 2.4485150321588147
epoch 8000 train : 0.29674521156464306 1.3840082432552843 2.239493364413838
epoch 8000 test : 0.28594658361856434 1.447731127035319 2.4664045307800735
epoch 8200 train : 0.2921175867779641 1.3723804446126682 2.229575932760996
epoch 8200 test : 0.2814156527822359 1.4358321033126333 2.4509470343138395
epoch 8400 train : 0.29641462826134696 1.372562766292775 2.2254464148869775
epoch 8400 test : 0.2864984221071667 1.4515904187588873 2.4653706512102427
epoch 8600 train : 0.28874893385965433 1.3784993594990202 2.2358621787436004
epoch 8600 test : 0.2759592060290957 1.4463941162478995 2.462064445196152
epoch 8800 train : 0.2895973076034371 1.361549499248512 2.218340528358756
epoch 8800 test : 0.27695703493633483 1.4126702723388356 2.4373463283760257
epoch 9000 train : 0.29684629291060904 1.3698866128809815 2.222185741602432
epoch 9000 test : 0.28427173065575884 1.4455792374188297 2.4582203674178817
epoch 9200 train : 0.295292932458685 1.3732873862365949 2.2293729547723102
epoch 9200 test : 0.28365024200891165 1.4481544856408812 2.4606244390546603
epoch 9400 train : 0.29397647547359557 1.3568191235477973 2.214275640028502
epoch 9400 test : 0.281191126686401 1.406188005069794 2.42456896438839
epoch 9600 train : 0.29933373689994114 1.3703058628006695 2.219613508811552
epoch 9600 test : 0.28546116242192654 1.413819476309268 2.427180262029551
epoch 9800 train : 0.2925391825611513 1.3624450065277534 2.2187460723697443
epoch 9800 test : 0.28135254082778455 1.4273936161952399 2.4447680924181934
epoch 10000 train : 0.3055370369987828 1.385952088932715 2.232729759879475
epoch 10000 test : 0.2924766543227263 1.4465957006347836 2.4531723559422542
epoch 10200 train : 0.28255607619999634 1.386642428465201 2.2471039075217276
epoch 10200 test : 0.2717473282077408 1.4656147925594107 2.4829989146691833
epoch 10400 train : 0.28242113336681324 1.354346175468843 2.2122120373561636
epoch 10400 test : 0.27190456941859925 1.403994429752479 2.4243922051608195
epoch 10600 train : 0.2886169881658101 1.3455037931770863 2.204650998224128
epoch 10600 test : 0.2782807694645041 1.4067603170298228 2.4275813882807125
epoch 10800 train : 0.2908231396093355 1.3645616143542199 2.221842452370323
epoch 10800 test : 0.27825135189720573 1.411330459106485 2.4314833336917436
epoch 11000 train : 0.2979099689721484 1.3638868783503824 2.218694875253256
epoch 11000 test : 0.2858886688070998 1.425589052174349 2.442938764682545
epoch 11200 train : 0.2902549483659864 1.3423609092819508 2.1985135427533824
epoch 11200 test : 0.2780130942781126 1.3883727025284638 2.4113861930858174
epoch 11400 train : 0.29861146552992895 1.3901281099867346 2.242911571377825
epoch 11400 test : 0.2844375201140898 1.4757689836760766 2.4856438528880282
epoch 11600 train : 0.28045180158939603 1.352909372073993 2.212897465714205
epoch 11600 test : 0.2677049299797013 1.4005353327110004 2.423991223834951
epoch 11800 train : 0.29251811106634745 1.3524680446139699 2.2071905245627685
epoch 11800 test : 0.28232037113313324 1.4303755328442749 2.4467647512764255
epoch 12000 train : 0.28754486616933433 1.3383662764812934 2.1963132686023843
epoch 12000 test : 0.2760235175634072 1.3856408424358637 2.4087448876305264
epoch 12200 train : 0.28366379869949665 1.3436952948644878 2.2041618647192225
epoch 12200 test : 0.27179581514094475 1.4030656344900683 2.4253119881281737
epoch 12400 train : 0.293377925834253 1.3651570451646577 2.221808062671154
epoch 12400 test : 0.28034057652906785 1.4230384289570834 2.441244660775216
epoch 12600 train : 0.291411058921392 1.3362146023226673 2.190684723151369
epoch 12600 test : 0.27937804122189475 1.390204405374232 2.4085239599403034
epoch 12800 train : 0.29143265750519687 1.341345629923637 2.1972426193136845
epoch 12800 test : 0.2784356875211431 1.394260732669125 2.4125506872429936
epoch 13000 train : 0.2837084638531525 1.3410331421426251 2.1979372345881085
epoch 13000 test : 0.2706140140434384 1.3922847863087218 2.4098244596674734
epoch 13200 train : 0.2894028985335254 1.3736010282769575 2.229319503067904
epoch 13200 test : 0.2780200395127309 1.4598237220793358 2.470281435126682
epoch 13400 train : 0.28462325507992486 1.328061822281925 2.1895801552785517
epoch 13400 test : 0.2733573740456708 1.3878899097052904 2.4103253712605865
epoch 13600 train : 0.2941852419095825 1.3387091124013095 2.1908279723191537
epoch 13600 test : 0.2825348150971567 1.3995587974787327 2.4152594842537063
epoch 13800 train : 0.28534000368306556 1.3298497524051454 2.187975487922122
epoch 13800 test : 0.27304243078381957 1.3847893656059613 2.402556584363824
epoch 14000 train : 0.2970015984632247 1.3416060243023011 2.198708317242865
epoch 14000 test : 0.28089565475927886 1.3748726540053893 2.4035332018533837
epoch 14200 train : 0.28657073269334016 1.3342178817597856 2.195123721821093
epoch 14200 test : 0.27478143610311817 1.4003072667729834 2.4269298426985517
epoch 14400 train : 0.2820818758037205 1.3255791102851213 2.18795097325253
epoch 14400 test : 0.2692622330289427 1.3738522292697748 2.3985642581017843
epoch 14600 train : 0.29471257551563224 1.329391681771546 2.1824830418174037
epoch 14600 test : 0.2817046765083856 1.3855166321549415 2.4000460677577777
epoch 14800 train : 0.2904231461906783 1.3280213215054937 2.185518383480363
epoch 14800 test : 0.27714077473524334 1.3809293424797457 2.4030985098879705
epoch 15000 train : 0.28384711956873776 1.3488751476825893 2.2091437410851795
epoch 15000 test : 0.26890774131896145 1.421335726529683 2.444528249324815
epoch 15200 train : 0.2898585847337048 1.3350421414684868 2.194019441087653
epoch 15200 test : 0.27666176092128225 1.3933745889556628 2.413600047646265
epoch 15400 train : 0.28573412971802176 1.3190169061091963 2.1762143693340734
epoch 15400 test : 0.2724328756886503 1.3797399332188083 2.399719078380177
epoch 15600 train : 0.28873784692733745 1.3296034957457898 2.1852196565978104
epoch 15600 test : 0.27477322315592556 1.3782488813895624 2.3989456266722597
epoch 15800 train : 0.28715260058041925 1.3228027766633916 2.183670385499121
epoch 15800 test : 0.2742899772394565 1.381786629093169 2.407807548763415
epoch 16000 train : 0.2821220776242672 1.3211735362407506 2.185008115865896
epoch 16000 test : 0.2706893938923263 1.3766305267106966 2.4056854434749186
epoch 16200 train : 0.2892505658349878 1.3135199307635612 2.169556365519748
epoch 16200 test : 0.27646019761413243 1.3717907412691877 2.390714363558307
epoch 16400 train : 0.2828606146810426 1.3201696079406504 2.181777900181506
epoch 16400 test : 0.2704539624225205 1.382573788640681 2.408693379911138
epoch 16600 train : 0.29070298913448717 1.3265992850361619 2.182497141028611
epoch 16600 test : 0.2788154168284512 1.3813433429844222 2.399583698315892
epoch 16800 train : 0.2858343648046455 1.3252097675012329 2.1841835963559593
epoch 16800 test : 0.2741620846252391 1.3730631238587536 2.399173500202859
epoch 17000 train : 0.2843579088166262 1.321825639997868 2.182351581519776
epoch 17000 test : 0.27335316832463097 1.3943904555249929 2.413735670341067
epoch 17200 train : 0.283741603668179 1.3110763077083303 2.172354458361458
epoch 17200 test : 0.2690703934169233 1.3573858453721874 2.3839399283964586
epoch 17400 train : 0.29022231938283904 1.326376804604425 2.18223565155966
epoch 17400 test : 0.27734664304785184 1.3698401609042798 2.3915493271740393
epoch 17600 train : 0.2901309177820145 1.3345925049134502 2.19514890916128
epoch 17600 test : 0.27615489963178136 1.3943497879110591 2.4209786570851493
epoch 17800 train : 0.28926744012084266 1.3432620468245364 2.204289629512352
epoch 17800 test : 0.2756506721216844 1.386916477458435 2.4096707052214956
epoch 18000 train : 0.29079242331862576 1.3396086612985343 2.1986908078506353
epoch 18000 test : 0.27746242933646925 1.3978435828815015 2.4160963913495754
epoch 18200 train : 0.2831541607189015 1.3209243271450368 2.1792924285893025
epoch 18200 test : 0.26906004431279773 1.3908162861182871 2.4069340160571246
epoch 18400 train : 0.2836421720516046 1.3108957420071992 2.1709498986968385
epoch 18400 test : 0.26969657209207637 1.357038507505182 2.3856091257586716
epoch 18600 train : 0.2851410452124811 1.32750100909959 2.1922495228496452
epoch 18600 test : 0.27322266362132946 1.4075178222321791 2.434935606306184
epoch 18800 train : 0.2909083587045586 1.3278442961934673 2.185421160615404
epoch 18800 test : 0.27760619958783234 1.388722473514049 2.409321623425325
epoch 19000 train : 0.2876502107280586 1.3293726081343096 2.1843719838932074
epoch 19000 test : 0.2753320989194166 1.382104947738828 2.396704433858302
epoch 19200 train : 0.28742277101573405 1.3275572301736913 2.185584111866563
epoch 19200 test : 0.2734534238687724 1.39083599004604 2.4127804098356522
epoch 19400 train : 0.28671653423975846 1.3275840240743615 2.1899078240480967
epoch 19400 test : 0.2748270414920438 1.3724254800388977 2.3987547758309136
epoch 19600 train : 0.28337734760819827 1.3158793770943407 2.175833921485847
epoch 19600 test : 0.2733384093717705 1.378263568968722 2.399061909670408
epoch 19800 train : 0.2780989495655897 1.3231835833687793 2.184002535775325
epoch 19800 test : 0.2660855778611849 1.392193355296129 2.412246884873657
epoch 20000 train : 0.28463684213570567 1.3191445143484712 2.1793663883357293
epoch 20000 test : 0.2735132495852531 1.3859467432983592 2.407561533049705

"""